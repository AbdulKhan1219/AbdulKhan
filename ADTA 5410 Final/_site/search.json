[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello, this is my introduction post regarding my work in the Master course for Advanced Data Analytics. In these posts I will be show casing some of the work I have done and how I did them and my comments to go along with any of the assignments. Thanks!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abdul Khan",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nAbdul Khan\n\n\n\n\n\n\n  \n\n\n\n\nIntro to Linear Regression in RStudio\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nAbdul Khan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Abdul Khan",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Intro to Linear Regression in RStudio/index.html",
    "href": "posts/Intro to Linear Regression in RStudio/index.html",
    "title": "Intro to Linear Regression in RStudio",
    "section": "",
    "text": "Hello, to begin the Linear Regression in RStudio we first have to load in the libraries we will be using for this example. With the code block provided below it will import all the necessary libraries.\n\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rpart)\nlibrary(rsample)\nlibrary(caret)\nlibrary(mgcv)\n\nknitr::opts_chunk$set(echo = TRUE)\n\nThe data that we will be using in this example will be from 2004 North Carolina Birth Records. In the dataset we are trying to determine if there is any relationship between the behaviours of pregnant women and the outcome of their childbirth.\n\n# Run this code chunk without altering it\n# clear the session\nrm(list=ls())\n\n# Data is stored in a csv file, the first row contains the variable names. \n# we call our data mydata\nmydata&lt;-read.csv (\"Data_RLab5.csv\", header=TRUE)\n\n# remove lowbirthweight\nmydata&lt;-mydata%&gt;%\n  select(-lowbirthweight)\n\nRun the following code block above to remove some necessary variables we will not be taking into consideration in our analysis.\n\n#With the below code we will be examining the variable descriptions and the overall structure of the dataset\nstr(mydata)\n\n'data.frame':   999 obs. of  12 variables:\n $ fage    : int  NA NA 19 21 NA NA 18 17 NA 20 ...\n $ mage    : int  13 14 15 15 15 15 15 15 16 16 ...\n $ mature  : chr  \"younger mom\" \"younger mom\" \"younger mom\" \"younger mom\" ...\n $ weeks   : int  39 42 37 41 39 38 37 35 38 37 ...\n $ premie  : chr  \"full term\" \"full term\" \"full term\" \"full term\" ...\n $ visits  : int  10 15 11 6 9 19 12 5 9 13 ...\n $ marital : chr  \"married\" \"married\" \"married\" \"married\" ...\n $ gained  : int  38 20 38 34 27 22 76 15 NA 52 ...\n $ weight  : num  7.63 7.88 6.63 8 6.38 5.38 8.44 4.69 8.81 6.94 ...\n $ gender  : chr  \"male\" \"male\" \"female\" \"male\" ...\n $ habit   : chr  \"nonsmoker\" \"nonsmoker\" \"nonsmoker\" \"nonsmoker\" ...\n $ whitemom: chr  \"not white\" \"not white\" \"white\" \"white\" ...\n\n#The next step is to find any missing values within the dataset so we will run the following code to determine what is missing\nmissing_values &lt;- colSums(is.na(mydata))\n\n\n#In this step we are replacing the missing values, for numeric variables we fill the missing values with the median, for categorical variables we use the mode to replace the missing values\nfor (col in names(mydata)) {\n  if (is.numeric(mydata[[col]])) {\n    mydata[[col]][is.na(mydata[[col]])] &lt;- median(mydata[[col]], na.rm = TRUE)\n  } else {\n    mode_val &lt;- names(sort(table(mydata[[col]]), decreasing = TRUE))[1]\n    mydata[[col]][is.na(mydata[[col]])] &lt;- mode_val\n  }\n}\n\n#In this step  we are are going to exclude all non-nomerical variables in order to calculate our highest correlations with the target variables\nnumeric_vars &lt;- sapply(mydata, is.numeric)\ncorrelations &lt;- cor(mydata[, numeric_vars])\n\ntarget_corr &lt;- correlations[,\"weight\"]\nmax_corr_var &lt;- names(target_corr[which.max(abs(target_corr))])\n\nlibrary(ggplot2)\n\n#With the below code block we are building a scatter plot \nggplot(mydata, aes(x = mydata[[max_corr_var]], y = weight)) +\n  geom_point() +\n  labs(title = paste(\"Scatter Plot\", max_corr_var, \"vs. Weight\"),\n       x = max_corr_var,\n       y = \"Weight\")\n\nWarning: Use of `mydata[[max_corr_var]]` is discouraged.\nℹ Use `.data[[max_corr_var]]` instead.\n\n\n\n\n\nThe scatter plot illustrates the connection between the variable ‘gained’—indicating the mother’s weight gain during pregnancy—and the target variable ‘weight’, which is the baby’s birth weight. This relationship is the most correlated in the dataset.\nKey Points:\nLinear Correlation: The plot demonstrates a notably strong linear correlation between the mother’s weight gain (‘gained’) and the baby’s birth weight (‘weight’). A higher weight gain by the mother generally corresponds to an increased birth weight of the baby.\nPositive Trend: There’s a positive trend observable in the plot, signifying a direct relationship where more weight gain during pregnancy relates to a heavier birth weight. This supports the usual belief linking maternal weight gain to the baby’s birth weight.\nPresence of Outliers: Some points on the plot stray from the primary trend, reflecting variations in the relationship. These could result from individual health differences, genetic influences, or personal lifestyle factors.\nIndication of a Linear Model: The scatter plot suggests a straight-line relation, indicative of a linear connection. Nevertheless, it’s crucial to recognize that while the trend appears linear, the actual relationship might not be entirely straight. Exploring different models might be beneficial to account for possible non-linear aspects.\nTo sum up, the scatter plot sheds light on how the weight gained during pregnancy impacts the baby’s birth weight. Further examination, like applying regression analysis, can offer a more detailed and accurate understanding of this relationship.\n\n# split the sample by using rsample package\n\n# Split the data into a training set (70%) and a test set (30%)\nset.seed(123456)\n# Set seed for reproducibility\nset.seed(123456)\n\n\n# Use initial_split to divide the data\nsplit_data &lt;- initial_split(mydata, prop = 0.7, strata = \"weight\")\n\n# Create training and test sets\ntrain_data &lt;- training(split_data)\ntest_data &lt;- testing(split_data)\n\n\n# Please provide your code for Task 3  in this code chunk\n# Run linear regression on the training data\nlinearmodel &lt;- lm(weight ~ ., data = train_data)\n\n# Predict weight in the test_data dataset\npredicted_weights_ols &lt;- predict(linearmodel, newdata = test_data)\n\n# Calculate Mean Squared Prediction Error (MSPE)\nMSPE_linear &lt;- mean((predicted_weights_ols - test_data$weight)^2)\n\n# Print the value of MSPE_linear to the console\nprint(MSPE_linear)\n\n[1] 1.133044\n\n\n\n#We will import another library for our analysis\nlibrary(mgcv)\n\n# Fit a GAM on the training data\ngam_model &lt;- gam(weight ~ s(fage) + s(mage) + s(weeks) + s(visits) + s(gained), data = train_data, method = \"REML\")\n\n# Print smoothing parameters\nsummary(gam_model)$s.table\n\n               edf   Ref.df           F     p-value\ns(fage)   1.000467 1.000932   3.6668451 0.055881477\ns(mage)   1.000709 1.001417   0.1991046 0.655874604\ns(weeks)  4.840498 5.882476 108.1701450 0.000000000\ns(visits) 4.265893 5.260296   3.1469249 0.007198697\ns(gained) 1.000058 1.000117   5.0806504 0.024504483\n\n# Predict weight in the test_data dataset using gam_model\npredicted_weights_gam &lt;- predict(gam_model, newdata = test_data)\n\n# Calculate MSPE\nMSPE_gam &lt;- mean((predicted_weights_gam - test_data$weight)^2)\n\n# Print the value of MSPE_gam to the console\nprint(MSPE_gam)\n\n[1] 1.159198\n\n\n\nMSPE_linear &lt;- mean((predicted_weights_ols - test_data$weight)^2)\n\nMSPE_gam &lt;- mean((predicted_weights_gam - test_data$weight)^2)\n\ncat(\"Linear Regression Model MSPE:\", MSPE_linear, \"\\n\")\n\nLinear Regression Model MSPE: 1.133044 \n\ncat(\"Generalized Additive Model MSPE:\", MSPE_gam, \"\\n\")\n\nGeneralized Additive Model MSPE: 1.159198 \n\nif (MSPE_linear &lt; MSPE_gam) {\n  cat(\"Conclusion: Linear Regression Model better performing based on MSPE.\\n\")\n} else if (MSPE_gam &lt; MSPE_linear) {\n  cat(\"Conclusion: Generalized Additive Model better performing based on MSPE.\\n\")\n} else {\n  cat(\"Conclusion: Both models performance are equal based on MSPE.\\n\")\n}\n\nConclusion: Linear Regression Model better performing based on MSPE.\n\n\nThe Mean Squared Prediction Errors (MSPE) were assessed for both the linear regression model (referred to as linearmodel) and the generalized additive model (referred to as gam_model), utilizing the test_data dataset.\nCalculated MSPE Values:\nMSPE for Linear Model: 1.133044 #MSPE for GAM: 1.159198 #Analysis:\nThe linear regression model exhibits a lower MSPE in comparison to the generalized additive model. To be precise, the MSPE for the linear model is 1.133044, whereas for the GAM it is 1.159198.\nConclusion:\nGiven its lower MSPE, the linear regression model (linearmodel) shows better prediction accuracy for the ‘weight’ variable within the test_data dataset. Hence, based on the MSPE metric, the linear model is the more suitable choice in this particular scenario.\nHowever, it is important to remember that evaluating models typically involves multiple criteria, and the selection of the most appropriate model may vary based on the specific objectives and the nature of the data involved.\nWith this blog post we have a given insight in how to preform a Linear Regression in RStudio"
  }
]